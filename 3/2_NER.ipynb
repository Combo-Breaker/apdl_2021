{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Извлечение именованных сущностей </center></h1>\n",
    "\n",
    "<h3><center>(Named Entity Recognition, NER)</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spacy, Natasha\n",
    "- CNN-biLSTM-CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HSE\t\t0\t\t3\t\tORG\n",
      "Moscow\t\t24\t\t30\t\tGPE\n",
      "1 million rubles\t\t37\t\t53\t\tMONEY\n"
     ]
    }
   ],
   "source": [
    "nlp_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"HSE opens a building in Moscow worth 1 million rubles\"\n",
    "\n",
    "for ent in nlp_eng(text).ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_, sep = '\\t\\t')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Для хранения NER-разметки часто используется BIO-нотация:\n",
    "\n",
    "B - если сущность состоит из более чем одного токена, то у первого токена будет тип B-{TYPE}, например, B-LOC = первый токен сущности типа LOCATION\n",
    "\n",
    "I - токен является сущностю, например, I-LOC = сущность типа LOCATION\n",
    "\n",
    "O - токен не является сущностью\n",
    "\n",
    "Пример BIO-нотации:\n",
    "\n",
    "НИУ : B-ORG\n",
    "ВШЭ : I-ORG \n",
    "открывает : O \n",
    "корпус : O\n",
    "в : O \n",
    "Москве : I-LOC \n",
    ". : O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Kate', 0, 4, 'PERSON') ('Higher School of Economics', 33, 59, 'ORG')\n",
      "\n",
      "Kate\tB\tPERSON\n",
      "studies\tO\t\n",
      "computer\tO\t\n",
      "science\tO\t\n",
      "in\tO\t\n",
      "Higher\tB\tORG\n",
      "School\tI\tORG\n",
      "of\tI\tORG\n",
      "Economics\tI\tORG\n"
     ]
    }
   ],
   "source": [
    "doc = nlp_eng(\"Kate studies computer science in Higher School of Economics\")\n",
    "\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(*ents)\n",
    "print()\n",
    "\n",
    "for i in range(len(doc)):\n",
    "    print(*[doc[i].text, doc[i].ent_iob_, doc[i].ent_type_], sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.7/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">In \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    September 1972\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jobs\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " enrolled at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Reed College\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Portland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Oregon\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "text = \"In September 1972, Jobs enrolled at Reed College in Portland, Oregon.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данный момент нет модели для русского языка :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import natasha\n",
    "from natasha import Doc, NewsEmbedding, NewsNERTagger, MorphVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = NewsEmbedding()\n",
    "\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В феврале 1974 года Стив Джобс устроился техником в молодую компанию \n",
      "                    PER───────                                       \n",
      "Атари в Лос-Гатосе (Калифорния)\n",
      "PER──   LOC───────  LOC─────── \n"
     ]
    }
   ],
   "source": [
    "text = 'В феврале 1974 года Стив Джобс устроился техником в молодую компанию Атари в Лос-Гатосе (Калифорния)'\n",
    "markup = ner_tagger(text)\n",
    "markup.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть отдельные парсеры для разных типов сущностей, они все являются обертками над парсером Yargy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MorphForm(normal='стекло', pos='NOUN', feats={'Animacy': 'Inan', 'Gender': 'Neut', 'Number': 'Sing', 'Case': 'Nom'}),\n",
       " MorphForm(normal='стекло', pos='NOUN', feats={'Animacy': 'Inan', 'Gender': 'Neut', 'Number': 'Sing', 'Case': 'Acc'}),\n",
       " MorphForm(normal='стечь', pos='VERB', feats={'VerbForm': 'Fin', 'Aspect': 'Perf', 'Gender': 'Neut', 'Number': 'Sing', 'Tense': 'Past', 'Mood': 'Ind'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph_vocab = MorphVocab() #обертка для Pymorphy2\n",
    "\n",
    "morph_vocab('стекло')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсер для имен (согласно документации, его лучше применять к спанам текста):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Match(\n",
       "     start=12,\n",
       "     stop=22,\n",
       "     fact=Name(\n",
       "         first=None,\n",
       "         last='директором',\n",
       "         middle=None\n",
       "     )\n",
       " ),\n",
       " Match(\n",
       "     start=38,\n",
       "     stop=48,\n",
       "     fact=Name(\n",
       "         first='Эд',\n",
       "         last='Кэтмелл',\n",
       "         middle=None\n",
       "     )\n",
       " ),\n",
       " Match(\n",
       "     start=50,\n",
       "     stop=51,\n",
       "     fact=Name(\n",
       "         first=None,\n",
       "         last='а',\n",
       "         middle=None\n",
       "     )\n",
       " ),\n",
       " Match(\n",
       "     start=52,\n",
       "     stop=62,\n",
       "     fact=Name(\n",
       "         first=None,\n",
       "         last='креативный',\n",
       "         middle=None\n",
       "     )\n",
       " ),\n",
       " Match(\n",
       "     start=81,\n",
       "     stop=94,\n",
       "     fact=Name(\n",
       "         first='Джон',\n",
       "         last='Лассетер',\n",
       "         middle=None\n",
       "     )\n",
       " )]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from natasha import NamesExtractor\n",
    "\n",
    "names_extractor = NamesExtractor(morph_vocab)\n",
    "\n",
    "text = 'Генеральным директором Pixar является Эд Кэтмелл, а креативный отдел возглавляет Джон Лассетер'\n",
    "list(names_extractor(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(start=12, stop=20, fact=Name(first=None, last='директор', middle=None))\n",
      "Match(start=0, stop=13, fact=Name(first='Джон', last='Лассетер', middle=None))\n",
      "Match(start=0, stop=12, fact=Name(first='А', last='Пушкин', middle='С'))\n",
      "Match(start=0, stop=9, fact=Name(first=None, last='Лермонтов', middle=None))\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    'генеральный директор Эд Кэтмелл',\n",
    "    'Джон Лассетер',\n",
    "    'А. С. Пушкин',\n",
    "    'Лермонтов'\n",
    "]\n",
    "\n",
    "for line in text:\n",
    "    print(names_extractor.find(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(start=0, stop=10, fact=Date(year=2017, month=1, day=24))\n",
      "Match(start=12, stop=20, fact=Date(year=2015, month=None, day=None))\n",
      "Match(start=22, stop=28, fact=Date(year=2014, month=None, day=None))\n",
      "Match(start=30, stop=38, fact=Date(year=None, month=4, day=1))\n",
      "Match(start=40, stop=51, fact=Date(year=2017, month=5, day=None))\n",
      "Match(start=53, stop=68, fact=Date(year=2017, month=5, day=9))\n"
     ]
    }
   ],
   "source": [
    "from natasha import DatesExtractor\n",
    "\n",
    "dates_extractor = DatesExtractor(morph_vocab)\n",
    "\n",
    "text = '24.01.2017, 2015 год, 2014 г, 1 апреля, май 2017 г., 9 мая 2017 года'\n",
    "print(*list(dates_extractor(text)), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Деньги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(start=2, stop=16, fact=Money(amount=23100, currency='RUB'))\n",
      "Match(start=18, stop=36, fact=Money(amount=1565321, currency='USD'))\n",
      "Match(start=38, stop=50, fact=Money(amount=34000, currency='EUR'))\n",
      "Match(start=92, stop=107, fact=Money(amount=13.32, currency='RUB'))\n"
     ]
    }
   ],
   "source": [
    "from natasha import MoneyExtractor\n",
    "\n",
    "money_extractor = MoneyExtractor(morph_vocab)\n",
    "\n",
    "text = \"$ 23, 100 рублей, 1,565,321 долларов, 34 тыс. евро, тринадцать рублей тридцать две копейки, 13 руб. 32 коп.\"\n",
    "print(*list(money_extractor(text)), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Адреса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match(start=0, stop=47, fact=Addr(parts=[AddrPart(value='Россия', type='страна'), AddrPart(value='Москва', type=None), AddrPart(value='Тверская', type='улица'), AddrPart(value='5', type='дом'), AddrPart(value='3', type='корпус')]))\n",
      "\n",
      "Match(start=0, stop=56, fact=Addr(parts=[AddrPart(value='108845', type='индекс'), AddrPart(value='РФ', type='страна'), AddrPart(value='Приморский', type='край'), AddrPart(value='Находка', type='город'), AddrPart(value='Добролюбова', type='улица')]))\n",
      "\n",
      "Match(start=0, stop=39, fact=Addr(parts=[AddrPart(value='Солнченый', type='посёлок'), AddrPart(value='Никитская', type='улица'), AddrPart(value='2', type='дом')]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from natasha import AddrExtractor\n",
    "\n",
    "addr_extractor = AddrExtractor(morph_vocab)\n",
    "\n",
    "lines = [\n",
    "    'Россия, Москва, Тверская улица, дом 5, корпус 3',\n",
    "    '108845, РФ, Приморский край, г. Находка, ул. Добролюбова, 18',\n",
    "    'поселок Солнченый, ул. Никитская, дом 2'\n",
    "]\n",
    "for line in lines:\n",
    "    print(addr_extractor.find(line), end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-biLSTM-CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы воспользуемся кодом и предобученными моделями отсюда: https://github.com/jayavardhanr/End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial\n",
    "\n",
    "В репозитории воспроизведена архитектура BiLSTM-CNN-CRF, модель обучена на данных соревнования CoNLL 2003 shared task (англоязычный корпус).\n",
    "\n",
    "В данных размечены четыре типа сущностей:\n",
    "\n",
    "- (PER) PERSON\n",
    "\n",
    "- (LOC) LOCATION\n",
    "\n",
    "- (ORG) ORGANIZATION\n",
    "\n",
    "- (MISC) MISCELLANEOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial'...\n",
      "remote: Enumerating objects: 57, done.\u001b[K\n",
      "remote: Total 57 (delta 0), reused 0 (delta 0), pack-reused 57\u001b[K\n",
      "Unpacking objects: 100% (57/57), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jayavardhanr/End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам понадобятся предобученные вектора слов, воспользуемся векторами glove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget http://nlp.stanford.edu/data/glove.6B.zip && unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "from torch import autograd\n",
    "\n",
    "import time\n",
    "import _pickle as cPickle\n",
    "\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры для модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = OrderedDict()\n",
    "parameters['train'] = \"./End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial/data/eng.train\" # путь к обучающей выборке\n",
    "parameters['dev'] = \"./End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial/data/eng.testa\" # путь к тестовой выборке\n",
    "parameters['test'] = \"./End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial/data/eng.testb\" # путь к валидационной выборке\n",
    "parameters['tag_scheme'] = \"BIOES\" # BIO или BIOES\n",
    "parameters['lower'] = True # все токены переводим в нижний регистр\n",
    "parameters['zeros'] =  True # все числа заменяем на 0 \n",
    "parameters['char_dim'] = 30 # размерность символьных эмбеддингов\n",
    "parameters['word_dim'] = 100 # размерность символьных слов (в нашем случсае glove)\n",
    "parameters['word_lstm_dim'] = 200 # размерность скрытого слоя LSTM \n",
    "parameters['word_bidirect'] = True # используем двунаправленную LSTM\n",
    "parameters['embedding_path'] = \"glove.6B.100d.txt\" # путь к модели с предобученными векторами слов\n",
    "parameters['all_emb'] = 1 # загружаем все эмбеддинги\n",
    "parameters['crf'] =1 # используем CRF (0 если не используем)\n",
    "parameters['dropout'] = 0.5 # Droupout \n",
    "parameters['epoch'] =  50 # число эпох обучения\n",
    "parameters['weights'] = \"\" # путь к предобученной модели (если сохраняем новую)\n",
    "parameters['name'] = \"self-trained-model\" # название модели\n",
    "parameters['gradient_clip'] = 5.0\n",
    "parameters['char_mode'] = \"CNN\"\n",
    "models_path = \"./End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial/models/\" # путь к предобученным моделям\n",
    "\n",
    "#GPU\n",
    "parameters['use_gpu'] = torch.cuda.is_available() \n",
    "use_gpu = parameters['use_gpu']\n",
    "\n",
    "parameters['reload'] = \"./End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial/models/pre-trained-model\" \n",
    "\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'\n",
    "mapping_file = './data/mapping.pkl'\n",
    "\n",
    "name = parameters['name']\n",
    "model_name = models_path + name \n",
    "\n",
    "if not os.path.exists(models_path):\n",
    "    os.makedirs(models_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем и предобрабатываем данные: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    заменяем все цифры на \"0\"\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = load_sentences(parameters['train'], parameters['zeros'])\n",
    "test_sentences = load_sentences(parameters['test'], parameters['zeros'])\n",
    "dev_sentences = load_sentences(parameters['dev'], parameters['zeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'NNP', 'I-NP', 'I-ORG'],\n",
       " ['rejects', 'VBZ', 'I-VP', 'O'],\n",
       " ['German', 'JJ', 'I-NP', 'I-MISC'],\n",
       " ['call', 'NN', 'I-NP', 'O'],\n",
       " ['to', 'TO', 'I-VP', 'O'],\n",
       " ['boycott', 'VB', 'I-VP', 'O'],\n",
       " ['British', 'JJ', 'I-NP', 'I-MISC'],\n",
       " ['lamb', 'NN', 'I-NP', 'O'],\n",
       " ['.', '.', 'O', 'O']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертируем BIO разметку в BIOES:\n",
    "\n",
    "B, I, E - если сущность состоит из более чем одного токена, то у первого токена будет тип B-{TYPE}, у последнего - E-{TYPE}, у всех остальных - I-{TYPE}. \n",
    "\n",
    "O - токен не является сущностью\n",
    "\n",
    "S - токен является сущностью и состоит только из этого токена\n",
    "\n",
    "Пример BIO-нотации:\n",
    "\n",
    "ФКН : B-ORG\n",
    "НИУ : I-ORG\n",
    "ВШЭ : E-ORG \n",
    "открывает : O \n",
    "корпус : O\n",
    "в : O \n",
    "Москве : S-LOC \n",
    ". : O "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iob2(tags):\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  \n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  \n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    BIO -> BIOES \n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "               tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('B-', 'S-'))\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('I-', 'E-'))\n",
    "        else:\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n",
    "def update_tag_scheme(sentences, tag_scheme):\n",
    "    for i, s in enumerate(sentences):\n",
    "        tags = [w[-1] for w in s]\n",
    "        if not iob2(tags):\n",
    "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
    "            raise Exception('Sentences should be given in BIO format! ' +\n",
    "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
    "        if tag_scheme == 'BIOES':\n",
    "            new_tags = iob_iobes(tags)\n",
    "            for word, new_tag in zip(s, new_tags):\n",
    "                word[-1] = new_tag\n",
    "        else:\n",
    "            raise Exception('Wrong tagging scheme!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tag_scheme(train_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(dev_sentences, parameters['tag_scheme'])\n",
    "update_tag_scheme(test_sentences, parameters['tag_scheme'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим вспомогательные словари (маппинги)\n",
    "\n",
    "слово -> ID\n",
    "\n",
    "символ -> ID\n",
    "\n",
    "тег -> ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dico(item_list):\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    словарь (маппинг) для слов, отсортированный по частоте\n",
    "    \"\"\"\n",
    "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(f\"{len(dico)} уникальных слов\")\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def char_mapping(sentences):\n",
    "    \"\"\"\n",
    "    словарь (маппинг) для символов, отсортированный по частоте\n",
    "    \"\"\"\n",
    "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
    "    dico = create_dico(chars)\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print(f\"{len(dico)} уникальных символов\" )\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    словарь (маппинг) для тегов, отсортированный по частоте\n",
    "    \"\"\"\n",
    "    tags = [[word[-1] for word in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    dico[START_TAG] = -1\n",
    "    dico[STOP_TAG] = -2\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(f\"{len(dico)} уникальных тегов сущностей\")\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17493 уникальных слов\n",
      "75 уникальных символов\n",
      "19 уникальных тегов сущностей\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
    "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем наши данные в соответствии с этим маппингом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(x,lower=False):\n",
    "    if lower:\n",
    "        return x.lower()  \n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041 / 3250 / 3453 предложений в train / dev / test выборках\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "\n",
    "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data\n",
    "\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
    ")\n",
    "print(f\"{len(train_data)} / {len(dev_data)} / {len(test_data)} предложений в train / dev / test выборках\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем веткора слов glove:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузили 400000 векторов слов\n"
     ]
    }
   ],
   "source": [
    "all_word_embeds = {}\n",
    "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
    "    s = line.strip().split()\n",
    "    if len(s) == parameters['word_dim'] + 1:\n",
    "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
    "\n",
    "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
    "\n",
    "for w in word_to_id:\n",
    "    if w in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
    "    elif w.lower() in all_word_embeds:\n",
    "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
    "\n",
    "print(f'Загрузили {len(all_word_embeds)} векторов слов')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель**\n",
    "\n",
    "Для инициализации слоя эмбеддингов используем  распределение  $-\\sqrt{\\frac{3}{V}}$ to $+\\sqrt{\\frac{3}{V}}$ где $V$ - размерность пространства эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding(input_embedding):\n",
    "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
    "    nn.init.uniform(input_embedding, -bias, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear(input_linear):\n",
    "\n",
    "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
    "    nn.init.uniform(input_linear.weight, -bias, bias)\n",
    "    if input_linear.bias is not None:\n",
    "        input_linear.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для инициализации LSTM используем распределение $-\\sqrt{\\frac{6}{r+c}}$ to $+\\sqrt{\\frac{6}{r+c}}$ где r и c - число столбцов и строк в матрице весов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_lstm(input_lstm):\n",
    "    \n",
    "    for ind in range(0, input_lstm.num_layers):\n",
    "    \n",
    "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
    "        \n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        \n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
    "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "        \n",
    "        \n",
    "    if input_lstm.bidirectional:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
    "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
    "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
    "    \n",
    "    if input_lstm.bias:\n",
    "        for ind in range(0, input_lstm.num_layers):\n",
    "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
    "            \n",
    "            bias.data.zero_()\n",
    "            \n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
    "            bias.data.zero_()\n",
    "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "            \n",
    "        if input_lstm.bidirectional:\n",
    "            for ind in range(0, input_lstm.num_layers):\n",
    "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
    "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
    "                bias.data.zero_()\n",
    "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CRF**\n",
    "\n",
    "Пусть $x$ - входная последовательность токенов, $y$ - последовательность тегов, тогда условная вероятность\n",
    "\n",
    "$$P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}$$\n",
    "\n",
    "где \n",
    "\n",
    "$$\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомогательные функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "def to_scalar(var):\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "def score_sentences(self, feats, tags):\n",
    "    r = torch.LongTensor(range(feats.size()[0]))\n",
    "    if self.use_gpu:\n",
    "        r = r.cuda()\n",
    "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "    else:\n",
    "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
    "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
    "\n",
    "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже реализация алгоритма прямого прохода и алгоритма Витерби для CRF. Алгоритм Витерби - это динамический алгоритм для поиска наиболее вероятной последовательности состояний (в нашем случае - тегов последовательности). \n",
    " \n",
    "\n",
    "$$ % <![CDATA[\n",
    "\\begin{align*}\n",
    "\\tilde{s}_t(y_t) &= \\operatorname{argmax}_{y_t, \\ldots, y_m} C(y_t, \\ldots, y_m)\\\\\n",
    "            &= \\operatorname{argmax}_{y_{t+1}} s_t [y_t] + T[y_{t}, y_{t+1}] + \\tilde{s}_{t+1}(y^{t+1})\n",
    "\\end{align*} %]]>$$\n",
    "\n",
    "Вероятность последовательности тегов:\n",
    "\n",
    "$$ \\mathbb{P}(y_1, \\ldots, y_m) = \\frac{e^{C(y_1, \\ldots, y_m)}}{Z} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_alg(self, feats):\n",
    "    \n",
    "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "    \n",
    "    forward_var = autograd.Variable(init_alphas)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "        \n",
    "    for feat in feats:\n",
    "        emit_score = feat.view(-1, 1)\n",
    "        tag_var = forward_var + self.transitions + emit_score\n",
    "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
    "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
    "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
    "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
    "    alpha = log_sum_exp(terminal_var)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_algo(self, feats):\n",
    "    backpointers = []\n",
    "\n",
    "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "    \n",
    "    forward_var = Variable(init_vvars)\n",
    "    if self.use_gpu:\n",
    "        forward_var = forward_var.cuda()\n",
    "    for feat in feats:\n",
    "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() \n",
    "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
    "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] \n",
    "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
    "        if self.use_gpu:\n",
    "            viterbivars_t = viterbivars_t.cuda()\n",
    "\n",
    "        forward_var = viterbivars_t + feat\n",
    "        backpointers.append(bptrs_t)\n",
    "\n",
    "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
    "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "    path_score = terminal_var[best_tag_id]\n",
    "    \n",
    "    best_path = [best_tag_id]\n",
    "    for bptrs_t in reversed(backpointers):\n",
    "        best_tag_id = bptrs_t[best_tag_id]\n",
    "        best_path.append(best_tag_id)\n",
    "\n",
    "    start = best_path.pop()\n",
    "    best_path.reverse()\n",
    "    return path_score, best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_calc(self, sentence, chars, chars2_length, d):\n",
    "    \n",
    "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
    "    if self.use_crf:\n",
    "        score, tag_seq = self.viterbi_decode(feats)\n",
    "    else:\n",
    "        score, tag_seq = torch.max(feats, 1)\n",
    "        tag_seq = list(tag_seq.cpu().data)\n",
    "\n",
    "    return score, tag_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можем переходить к непосредственно модели:\n",
    "\n",
    "1. На вход моодели подаются токены, из символов с помощью CNN получаем символьные эмбеддинги.\n",
    "2. Конкатенируем символьные эмбеддинги с glove векторами слов, подаем на вход BiLSTM.\n",
    "3. Выход BiLSTM слоя пропускаем через линейный слой, из пространства выходных векторов BiLSTM получаем вектор пространства тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
    "    \n",
    "    if self.char_mode == 'LSTM':\n",
    "        \n",
    "            chars_embeds = self.char_embeds(chars2).transpose(0, 1)\n",
    "            \n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, chars2_length)\n",
    "            \n",
    "            lstm_out, _ = self.char_lstm(packed)\n",
    "            \n",
    "            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "            \n",
    "            outputs = outputs.transpose(0, 1)\n",
    "            \n",
    "            chars_embeds_temp = Variable(torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2)))))\n",
    "            \n",
    "            if self.use_gpu:\n",
    "                chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "            \n",
    "            for i, index in enumerate(output_lengths):\n",
    "                chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))\n",
    "            \n",
    "            chars_embeds = chars_embeds_temp.clone()\n",
    "            \n",
    "            for i in range(chars_embeds.size(0)):\n",
    "                chars_embeds[d[i]] = chars_embeds_temp[i]\n",
    "    \n",
    "    \n",
    "    if self.char_mode == 'CNN':\n",
    "        chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
    "\n",
    "        chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
    "        chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
    "                                             kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
    "\n",
    "    embeds = self.word_embeds(sentence)\n",
    "\n",
    "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "\n",
    "    embeds = embeds.unsqueeze(1)\n",
    "\n",
    "    embeds = self.dropout(embeds)\n",
    "\n",
    "    lstm_out, _ = self.lstm(embeds)\n",
    "\n",
    "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "    lstm_feats = self.hidden2tag(lstm_out)\n",
    "    \n",
    "    return lstm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
    "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
    "\n",
    "    if self.use_crf:\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "    else:\n",
    "        tags = Variable(tags)\n",
    "        scores = nn.functional.cross_entropy(feats, tags)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
    "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=25,char_embedding_dim=25, use_gpu=False\n",
    "                 , use_crf=True, char_mode='CNN'):\n",
    "        '''     \n",
    "                vocab_size= размер словаря \n",
    "                tag_to_ix = словарь маппинга тегов\n",
    "                embedding_dim = размерность эмбеддингов слов\n",
    "                hidden_dim = размер скрытого LSTM слоя \n",
    "                char_to_ix = словарь маппинга символов\n",
    "                pre_word_embeds = маппинг эмбеддинг слова -> индекс слова\n",
    "                char_out_dimension = размерность выходного слоя символьной CNN \n",
    "                char_embedding_dim = размерность символьных эмбеддингов\n",
    "        '''\n",
    "        \n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        self.use_gpu = use_gpu\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.use_crf = use_crf\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.out_channels = char_out_dimension\n",
    "        self.char_mode = char_mode\n",
    "\n",
    "        if char_embedding_dim is not None:\n",
    "            self.char_embedding_dim = char_embedding_dim\n",
    "            \n",
    "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
    "            init_embedding(self.char_embeds.weight)\n",
    "            \n",
    "            if self.char_mode == 'LSTM':\n",
    "                self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
    "                init_lstm(self.char_lstm)\n",
    "                \n",
    "            if self.char_mode == 'CNN':\n",
    "                self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
    "\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if pre_word_embeds is not None:\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "    \n",
    "        self.dropout = nn.Dropout(parameters['dropout'])\n",
    "        \n",
    "        if self.char_mode == 'LSTM':\n",
    "            self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2, hidden_dim, bidirectional=True)\n",
    "        if self.char_mode == 'CNN':\n",
    "            self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
    "        \n",
    "        init_lstm(self.lstm)\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
    "        \n",
    "        init_linear(self.hidden2tag) \n",
    "\n",
    "        if self.use_crf:\n",
    "            self.transitions = nn.Parameter(\n",
    "                torch.zeros(self.tagset_size, self.tagset_size))\n",
    "            \n",
    "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "    _score_sentence = score_sentences\n",
    "    _get_lstm_features = get_lstm_features\n",
    "    _forward_alg = forward_alg\n",
    "    viterbi_decode = viterbi_algo\n",
    "    neg_log_likelihood = get_neg_log_likelihood\n",
    "    forward = forward_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:9: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  if __name__ == '__main__':\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:13: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  del sys.path[0]\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:20: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:23: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "/Library/Python/3.7/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
    "                   tag_to_ix=tag_to_id,\n",
    "                   embedding_dim=parameters['word_dim'],\n",
    "                   hidden_dim=parameters['word_lstm_dim'],\n",
    "                   use_gpu=use_gpu,\n",
    "                   char_to_ix=char_to_id,\n",
    "                   pre_word_embeds=word_embeds,\n",
    "                   use_crf=parameters['crf'],\n",
    "                   char_mode=parameters['char_mode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем предобученную модедль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "модель загружена: ./End-to-end-Sequence-Labeling-via-Bi-directional-LSTM-CNNs-CRF-Tutorial/models/pre-trained-model\n"
     ]
    }
   ],
   "source": [
    "if parameters['reload']:\n",
    "    if not os.path.exists(parameters['reload']):\n",
    "        model_url=\"https://github.com/TheAnig/NER-LSTM-CNN-Pytorch/raw/master/trained-model-cpu\"\n",
    "        urllib.request.urlretrieve(model_url, parameters['reload'])\n",
    "    model.load_state_dict(torch.load(parameters['reload']))\n",
    "    print(\"модель загружена:\", parameters['reload'])\n",
    "\n",
    "if use_gpu:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.015\n",
    "momentum = 0.9\n",
    "number_of_epochs = parameters['epoch'] \n",
    "decay_rate = 0.05\n",
    "gradient_clip = parameters['gradient_clip']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "losses = [] \n",
    "loss = 0.0 \n",
    "best_dev_F = -1.0 \n",
    "best_test_F = -1.0 \n",
    "best_train_F = -1.0\n",
    "all_F = [[0, 0, 0]]\n",
    "eval_every = len(train_data) \n",
    "plot_every = 2000 \n",
    "count = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще немного вспомогательных функций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunk_type(tok, idx_to_tag): \n",
    "    \"\"\"\n",
    "    \"B-PER\" -> (PER) и (B)\n",
    "    \"\"\"\n",
    "    \n",
    "    tag_name = idx_to_tag[tok]\n",
    "    tag_class = tag_name.split('-')[0]\n",
    "    tag_type = tag_name.split('-')[-1]\n",
    "    return tag_class, tag_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(seq, tags):\n",
    "    \"\"\"\n",
    "\n",
    "    Аргументы:\n",
    "        seq: [4, 4, 0, 0, ...] последовтельность меток класса\n",
    "        tags: dict[\"O\"] = 4\n",
    "\n",
    "    Возвращает:\n",
    "        list of (chunk_type, chunk_start, chunk_end)\n",
    "\n",
    "    Пример:\n",
    "        seq = [4, 5, 0, 3]\n",
    "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
    "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    default = tags[\"O\"]\n",
    "    \n",
    "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    chunk_type, chunk_start = None, None\n",
    "    for i, tok in enumerate(seq):\n",
    "        if tok == default and chunk_type is not None:\n",
    "            chunk = (chunk_type, chunk_start, i)\n",
    "            chunks.append(chunk)\n",
    "            chunk_type, chunk_start = None, None\n",
    "\n",
    "        elif tok != default:\n",
    "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
    "            if chunk_type is None:\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
    "                chunk = (chunk_type, chunk_start, i)\n",
    "                chunks.append(chunk)\n",
    "                chunk_type, chunk_start = tok_chunk_type, i\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    if chunk_type is not None:\n",
    "        chunk = (chunk_type, chunk_start, len(seq))\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(model, datas, best_F,dataset=\"Train\"):\n",
    "    '''\n",
    "    Функция для вычисления F1 score\n",
    "    '''\n",
    "\n",
    "    prediction = [] \n",
    "    save = False \n",
    "    new_F = 0.0 \n",
    "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
    "    \n",
    "    for data in datas:\n",
    "        ground_truth_id = data['tags']\n",
    "        words = data['str_words']\n",
    "        chars2 = data['chars']\n",
    "        \n",
    "        if parameters['char_mode'] == 'LSTM':\n",
    "            chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "            d = {}\n",
    "            for i, ci in enumerate(chars2):\n",
    "                for j, cj in enumerate(chars2_sorted):\n",
    "                    if ci == cj and not j in d and not i in d.values():\n",
    "                        d[j] = i\n",
    "                        continue\n",
    "            chars2_length = [len(c) for c in chars2_sorted]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2_sorted):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "        \n",
    "        \n",
    "        if parameters['char_mode'] == 'CNN':\n",
    "            d = {} \n",
    "\n",
    "            chars2_length = [len(c) for c in chars2]\n",
    "            char_maxl = max(chars2_length)\n",
    "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "            for i, c in enumerate(chars2):\n",
    "                chars2_mask[i, :chars2_length[i]] = c\n",
    "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "        dwords = Variable(torch.LongTensor(data['words']))\n",
    "        \n",
    "        if use_gpu:\n",
    "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "        else:\n",
    "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
    "        predicted_id = out\n",
    "    \n",
    "        lab_chunks = set(get_chunks(ground_truth_id,tag_to_id))\n",
    "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
    "                                         tag_to_id))\n",
    "\n",
    "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
    "        total_preds   += len(lab_pred_chunks)\n",
    "        total_correct += len(lab_chunks)\n",
    "    \n",
    "    # F1-Score\n",
    "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
    "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
    "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
    "\n",
    "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
    "\n",
    "    \n",
    "    if new_F>best_F:\n",
    "        best_F=new_F\n",
    "        save=True\n",
    "\n",
    "    return best_F, new_F, save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "\n",
    "def adjust_learning_rate(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение модели.**\n",
    "\n",
    "Мы загрузим предобученную модель, а код ниже позволит обучить модель с нуля и сохранить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters['reload']=False\n",
    "\n",
    "if not parameters['reload']:\n",
    "    tr = time.time()\n",
    "    model.train(True)\n",
    "    for epoch in range(1,number_of_epochs):\n",
    "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
    "            count += 1\n",
    "            data = train_data[index]\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = data['words']\n",
    "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
    "            tags = data['tags']\n",
    "            chars2 = data['chars']\n",
    "            \n",
    "            if parameters['char_mode'] == 'LSTM':\n",
    "                chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
    "                d = {}\n",
    "                for i, ci in enumerate(chars2):\n",
    "                    for j, cj in enumerate(chars2_sorted):\n",
    "                        if ci == cj and not j in d and not i in d.values():\n",
    "                            d[j] = i\n",
    "                            continue\n",
    "                chars2_length = [len(c) for c in chars2_sorted]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2_sorted):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "            \n",
    "            if parameters['char_mode'] == 'CNN':\n",
    "\n",
    "                d = {}\n",
    "\n",
    "                chars2_length = [len(c) for c in chars2]\n",
    "                char_maxl = max(chars2_length)\n",
    "                chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "                for i, c in enumerate(chars2):\n",
    "                    chars2_mask[i, :chars2_length[i]] = c\n",
    "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "\n",
    "            targets = torch.LongTensor(tags)\n",
    "\n",
    "            if use_gpu:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "            else:\n",
    "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
    "            loss += neg_log_likelihood.data[0] / len(data['words'])\n",
    "            neg_log_likelihood.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), gradient_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            if count % plot_every == 0:\n",
    "                loss /= plot_every\n",
    "                print(count, ': ', loss)\n",
    "                if losses == []:\n",
    "                    losses.append(loss)\n",
    "                losses.append(loss)\n",
    "                loss = 0.0\n",
    "\n",
    "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
    "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
    "                model.train(False)\n",
    "                best_train_F, new_train_F, _ = evaluating(model, train_data, best_train_F,\"Train\")\n",
    "                best_dev_F, new_dev_F, save = evaluating(model, dev_data, best_dev_F,\"Dev\")\n",
    "                if save:\n",
    "                    print(\"Saving Model to \", model_name)\n",
    "                    torch.save(model.state_dict(), model_name)\n",
    "                best_test_F, new_test_F, _ = evaluating(model, test_data, best_test_F,\"Test\")\n",
    "\n",
    "                all_F.append([new_train_F, new_dev_F, new_test_F])\n",
    "                model.train(True)\n",
    "\n",
    "            if count % len(train_data) == 0:\n",
    "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
    "\n",
    "    print(time.time() - tr)\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "if not parameters['reload']:\n",
    "    model.load_state_dict(torch.load(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на модель на примерах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : tag \n",
      "\n",
      "Higher : O\n",
      "School : O\n",
      "of : O\n",
      "Economics : O\n",
      "opens : O\n",
      "a : O\n",
      "building : O\n",
      "in : O\n",
      "Moscow : LOC\n",
      "\n",
      "\n",
      "Steve : ORG\n",
      "Jobs : O\n",
      "is : O\n",
      "the : O\n",
      "co-creator : O\n",
      "of : O\n",
      "the : O\n",
      "Macintosh, : MISC\n",
      "iPod, : O\n",
      "iPhone, : O\n",
      "iPad, : O\n",
      "and : O\n",
      "first : O\n",
      "Apple : ORG\n",
      "Stores : O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_testing_sentences = ['Higher School of Economics opens a building in Moscow ',\n",
    "                           'Steve Jobs is the co-creator of the Macintosh, iPod, iPhone, iPad, and first Apple Stores']\n",
    "\n",
    "\n",
    "# параметры\n",
    "lower=parameters['lower']\n",
    "\n",
    "# предобработка\n",
    "final_test_data = []\n",
    "for sentence in model_testing_sentences:\n",
    "    s=sentence.split()\n",
    "    str_words = [w for w in s]\n",
    "    words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>'] for w in str_words]\n",
    "    \n",
    "    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n",
    "    \n",
    "    final_test_data.append({\n",
    "        'str_words': str_words,\n",
    "        'words': words,\n",
    "        'chars': chars,\n",
    "    })\n",
    "\n",
    "# предскзание модели\n",
    "predictions = []\n",
    "print(\"word : tag \\n\")\n",
    "for data in final_test_data:\n",
    "    words = data['str_words']\n",
    "    chars2 = data['chars']\n",
    "\n",
    "    d = {} \n",
    "    \n",
    "    chars2_length = [len(c) for c in chars2]\n",
    "    char_maxl = max(chars2_length)\n",
    "    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
    "    for i, c in enumerate(chars2):\n",
    "        chars2_mask[i, :chars2_length[i]] = c\n",
    "    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
    "\n",
    "    dwords = Variable(torch.LongTensor(data['words']))\n",
    "\n",
    "    if use_gpu:\n",
    "        val,predicted_id = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
    "    else:\n",
    "        val,predicted_id = model(dwords, chars2_mask, chars2_length, d)\n",
    "\n",
    "    pred_chunks = get_chunks(predicted_id,tag_to_id)\n",
    "    temp_list_tags=['O']*len(words)\n",
    "    for p in pred_chunks:\n",
    "        temp_list_tags[p[1]]=p[0]\n",
    "        \n",
    "    for word,tag in zip(words,temp_list_tags):\n",
    "        print(word,':',tag)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
